{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f02e2af1-998e-4b33-8220-62270916d947",
   "metadata": {},
   "source": [
    "# Машинное обучение ранжированию с помощью библиотеки LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d0fd7-e327-4713-847c-3dfe4b2dba1d",
   "metadata": {},
   "source": [
    "В этом примере мы:\n",
    "\n",
    "- познакомимся с библиотекой **LightGBM**\n",
    "- научимся решать задачу машинного обучения ранжирования используя алгоритмы, реализованные в **LightGBM**\n",
    "\n",
    "Будем использовать стандартный датасет [MSLR](https://www.microsoft.com/en-us/research/project/mslr/)\n",
    "\n",
    "Подробности про формат датасета можно найти в соседнем тюториале *catboost_ltr.ipynb* по машинному обучению ранжированию с помощью библиотеки **CatBoost**, предполагаем что мы его уже прошли."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb9465-67dd-46e4-8bf3-1910ebd75cab",
   "metadata": {},
   "source": [
    "## Библиотека LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6509fb-2c89-47c6-976c-4548d6a60e8a",
   "metadata": {},
   "source": [
    "Полезные ссылки:\n",
    "- Полная документация: https://lightgbm.readthedocs.io/en/stable/\n",
    "- класс LGBMRanker: https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRanker.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74db45aa-1306-4dc6-ace5-c34e973cc171",
   "metadata": {},
   "source": [
    "## Пререквизиты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5045e6-040a-4863-a095-46cf2cfc1813",
   "metadata": {},
   "source": [
    "Импортируем все что нам понадобится для дальнейшей работы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1626b56-ac20-486e-90dc-087240c96dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgbm\n",
    "# We'll use catboost to download dataset and to compute metrics\n",
    "from catboost import datasets, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0470ece-0248-41bd-8ac9-971730df1a6d",
   "metadata": {},
   "source": [
    "## Датасет MSLR (Microsoft Learning to Rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c393310a-8f54-4eb4-9fb5-ab2775f28799",
   "metadata": {},
   "source": [
    "Загрузим встроенный в катбуст сабсет датасета MSLR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61225ba7-5615-4e17-93e3-cbf592d3e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = datasets.msrank_10k()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcef0d21-056f-42d8-a007-966444ab0996",
   "metadata": {},
   "source": [
    "Для удобства присвоим колонкам говорящие имена:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "027e3c18-6103-4914-a872-cbc7879fce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_column_names(num_features):\n",
    "    \"\"\"Generates column names for LETOR-like datasets\"\"\"\n",
    "    columns = ['label', 'qid']\n",
    "    for i in range(num_features):\n",
    "        column = f\"feature_{i+1}\"\n",
    "        columns.append(column)\n",
    "    return columns\n",
    "\n",
    "# Assign column names\n",
    "columns = generate_column_names(num_features=136)\n",
    "df_train.columns = columns\n",
    "df_test.columns = columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b7cb1b-4ad1-438f-b6d8-fd1676c34683",
   "metadata": {},
   "source": [
    "Теперь наши данные выглядят красивее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "916f781c-8850-4dae-b619-cdc7ac0cc025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  qid  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0    2.0    1          3          3          0          0          3   \n",
      "1    2.0    1          3          0          3          0          3   \n",
      "2    0.0    1          3          0          2          0          3   \n",
      "3    2.0    1          3          0          3          0          3   \n",
      "4    1.0    1          3          0          3          0          3   \n",
      "\n",
      "   feature_6  feature_7  feature_8  ...  feature_127  feature_128  \\\n",
      "0        1.0        1.0   0.000000  ...           62     11089534   \n",
      "1        1.0        0.0   1.000000  ...           54     11089534   \n",
      "2        1.0        0.0   0.666667  ...           45            3   \n",
      "3        1.0        0.0   1.000000  ...           56     11089534   \n",
      "4        1.0        0.0   1.000000  ...           64            5   \n",
      "\n",
      "   feature_129  feature_130  feature_131  feature_132  feature_133  \\\n",
      "0            2          116        64034           13            3   \n",
      "1            2          124        64034            1            2   \n",
      "2            1          124         3344           14           67   \n",
      "3           13          123        63933            1            3   \n",
      "4            7          256        49697            1           13   \n",
      "\n",
      "   feature_134  feature_135  feature_136  \n",
      "0            0            0          0.0  \n",
      "1            0            0          0.0  \n",
      "2            0            0          0.0  \n",
      "3            0            0          0.0  \n",
      "4            0            0          0.0  \n",
      "\n",
      "[5 rows x 138 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_train.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f1545d-0999-4bb2-9d72-9cd9b10fb9fc",
   "metadata": {},
   "source": [
    "Всего у нас:\n",
    "\n",
    "- 87 запросов и 10000 документов в трейне\n",
    "- 88 запросов и 10000 документов в тесте\n",
    "- 130 колонок: таргет (оценка асессора), id запроса (qid) и вектор из 128 фичей\n",
    "- таргет принимает значения в интервале \\[0,4\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8fddaa-d7ba-453a-aa5f-5bc834be9951",
   "metadata": {},
   "source": [
    "Теперь нам надо представить датасет в формате, который можно подавать на вход модели lightgbm.\n",
    "\n",
    "В отличие от catboost и xgboost, которые \"связывают\" документы в рамках одного запроса по ID запроса, lightgbm кодирует группы немного по-другому.\n",
    "\n",
    "Мы все еще делим датасет на 3 части, только вместо вектора **q** у нас теперь вектор **g**:\n",
    "\n",
    "- **y** -- вектор таргетов\n",
    "- **X** -- тензор из фичей\n",
    "- **g** -- вектор c размерами групп\n",
    "\n",
    "Мы предполагаем, что документы в **y** и **X** сгруппированы и отсортированы по qid, а вот **g** теперь просто содержит размеры соответсвующих групп, и его размер равен не количеству объектов, а количеству групп.\n",
    "\n",
    "Проще всего понять это на примере.\n",
    "\n",
    "Будем использовать тот факт, что наш датасет уже отсортирован по qid.\n",
    "\n",
    "Убедимся в этом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e672a4fc-7cf6-4ee5-a598-286dde967d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(df_train['qid'].is_monotonic_increasing)\n",
    "print(df_test['qid'].is_monotonic_increasing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70885edf-16c5-4e99-b376-61e72e1b2b93",
   "metadata": {},
   "source": [
    "Сконвертируем датасет в нужный формат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f15870f-9459-48d7-a69d-5cd26c23f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lightgbm_dataset(df):\n",
    "    y = df['label'].to_numpy()                       # Label: [0-4]\n",
    "    q = df['qid'].to_numpy().astype('uint32')        # Query Id\n",
    "    X = df.drop(columns=['label', 'qid']).to_numpy() # 136 features\n",
    "\n",
    "    num_docs_by_qid = df.groupby('qid')['label'].count()\n",
    "    g = num_docs_by_qid.to_numpy()\n",
    "    return (X, y, q, g)\n",
    "\n",
    "X_train, y_train, q_train, g_train = to_lightgbm_dataset(df_train)\n",
    "X_test, y_test, q_test, g_test = to_lightgbm_dataset(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4878ad0d-ec41-4911-91da-9cc955210e86",
   "metadata": {},
   "source": [
    "Посмотрим как выглядят вектора **q** и **g**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffc0f5ab-4ccb-4b04-a29d-fff7d4a0b618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets and queries:\n",
      "     y  q\n",
      "0  2.0  1\n",
      "1  2.0  1\n",
      "2  0.0  1\n",
      "3  2.0  1\n",
      "4  1.0  1\n",
      "\n",
      "Groups:\n",
      "     g\n",
      "0   86\n",
      "1  106\n",
      "2   92\n",
      "3  120\n",
      "4   59\n"
     ]
    }
   ],
   "source": [
    "print(\"Targets and queries:\")\n",
    "print(pd.DataFrame.from_dict({'y': y_train, 'q': q_train}).head(5))\n",
    "\n",
    "print(\"\\nGroups:\")\n",
    "print(pd.DataFrame.from_dict({'g': g_train}).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7034387e-4b52-4522-ab59-ccc004610d93",
   "metadata": {},
   "source": [
    "Вектор **q** нам также понадобится в дальнейшем для расчета метрик, поэтому to_lightgbm_dataset() возвращает сразу и его."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ae868e-319b-4070-adfb-ecce7ae94205",
   "metadata": {},
   "source": [
    "## Обучаем pointwise модель"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2cf9a4-0c23-4d99-bc92-7cf08a8bf37f",
   "metadata": {},
   "source": [
    "Теперь можно приступить непосредственно к обучению модели. Мы начнем с простой pointwise модели которая в качестве лосса использует обычное RMSE.\n",
    "\n",
    "Количество деревьев будем выбирать автоматически с использованием т.н. early stopping -- процесс обучения будем остановлен после того, как на валидационном множестве перестанет расти наша целевая метрика (в нашем случае это тоже RMSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0534f6-713f-4fa1-83c6-5d20104c1495",
   "metadata": {},
   "source": [
    "Создадим объект модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22758401-b1aa-4eee-a6d5-2dfc257ec7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgbm.LGBMRegressor(\n",
    "    n_estimators=1000, # maximum possible number of trees\n",
    "    random_state=22\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9414a4-c51b-41f1-8d7f-e7eb3fca9446",
   "metadata": {},
   "source": [
    "И зафитим ее на нашем обучающем множестве.  \n",
    "Т.к. мы используем early stopping то передадим в функцию fit() в качестве валидационного множества (параметр eval_set) наш тест-сет (строго говоря так делать нельзя, но мы в данном примере не стремимся к строгости, подробнее этот момент расписан в тюториале про машинное обучение ранжированию с помощью каибуста)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f093276-08d9-462a-af6a-39a7e6fd0f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20855\n",
      "[LightGBM] [Info] Number of data points in the train set: 10000, number of used features: 136\n",
      "[LightGBM] [Info] Start training from score 0.628200\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's l2: 0.543193\n",
      "Evaluated only: l2\n",
      "Model fit: elapsed = 0.509\n"
     ]
    }
   ],
   "source": [
    "# Fit\n",
    "start = timer()\n",
    "model.fit(X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    callbacks=[lgbm.early_stopping(stopping_rounds=100, first_metric_only=True)],\n",
    ")\n",
    "elapsed = timer() - start\n",
    "print(f\"Model fit: elapsed = {elapsed:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42913ae1-7037-453f-9a12-e1b4666d1bf3",
   "metadata": {},
   "source": [
    "Посмотрим, сколько деревьев содержит модель. Это немного сложнее чем было с катбустом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6a0b61d-0cc6-4671-97f5-5c2fe55f81a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: num_trees = 35 best_iteration = 35\n"
     ]
    }
   ],
   "source": [
    "num_trees = model.booster_.num_trees()\n",
    "print(f\"Model params: num_trees = {num_trees} best_iteration = {model.best_iteration_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52632b78-fe55-4d29-8ad9-6d372d74fe97",
   "metadata": {},
   "source": [
    "Видно, что наша модель содержит 35 деревьев (и все 35 будут использованы для предикта)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b40b16-abdf-4ad7-a452-daf875366e76",
   "metadata": {},
   "source": [
    "При желании, мы можем сохранить модель в текстовом формате:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd4b6fac-7747-4dd4-93d6-c035a8e7a299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7fcbc7965f50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file = \"/tmp/model.lgbm.txt\"\n",
    "\n",
    "# Save model\n",
    "model.booster_.save_model(model_file)\n",
    "\n",
    "# Load model\n",
    "# model = lgbm.Booster(model_file=model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb89c7a-0987-4831-98f1-989f75240acf",
   "metadata": {},
   "source": [
    "Получим предикты модели на тестовом множестве:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e00327b-7005-44dc-990f-bc9ec5573368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: y_hat_test.shape = (10000,)\n"
     ]
    }
   ],
   "source": [
    "y_hat_test = model.predict(X_test)\n",
    "print(f\"Predicted: y_hat_test.shape = {y_hat_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23a2f92-6d18-45b8-8a38-d8ca5c3238b1",
   "metadata": {},
   "source": [
    "Теперь, имея предикты, можно посчитать метрики качества, тут для удобства и единообразия будем использовать catboost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe53d6fb-d8a0-4e9b-8f8f-48977a5dcc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric = NDCG:top=10;type=Exp score = 0.408\n",
      "metric = DCG:top=10;type=Exp score = 8.236\n",
      "metric = MAP:top=10 score = 0.520\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(y_true, y_hat, q):\n",
    "    # List of metrics to evaluate\n",
    "    eval_metrics = ['NDCG:top=10;type=Exp', 'DCG:top=10;type=Exp', 'MAP:top=10']\n",
    "    \n",
    "    for eval_metric in eval_metrics:\n",
    "        scores = utils.eval_metric(y_true, y_hat, eval_metric, group_id=q)\n",
    "    \n",
    "        # Print scores\n",
    "        print(f\"metric = {eval_metric} score = {scores[0]:.3f}\")\n",
    "    \n",
    "# Compute metrics on test\n",
    "compute_metrics(y_test, y_hat_test, q_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f6fd2e-a19e-492b-aec1-d8824184f980",
   "metadata": {},
   "source": [
    "Видим, что значение NDCG на тесте получилось чуть хуже, чем у катбуста."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6234e160-40b5-4609-a01e-c1c0613fc175",
   "metadata": {},
   "source": [
    "## Обучаем pairwise модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f48a3c-095c-4db0-8d49-8ed7fd488625",
   "metadata": {},
   "source": [
    "Теперь проделаем все то же самое, но с использованием алгоритма LambdaRank.  \n",
    "\n",
    "Для этого вместо *LGBMRegressor* будем использовать класс *LGBMRanker*, которому в качестве objective передадим 'lambdarank'.  \n",
    "В качестве метрики для early stopping на этот раз будем использовать ndcg.\n",
    "\n",
    "Функция fit() теперь принимает на вход еще и информацию о группах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce16b132-99ce-499b-bcdd-539d67008f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20855\n",
      "[LightGBM] [Info] Number of data points in the train set: 10000, number of used features: 136\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's ndcg@10: 0.409668\n",
      "Evaluated only: ndcg@10\n",
      "Model fit: num_trees = 27 elapsed = 0.283\n",
      "Predicted: y_hat_test.shape = (10000,)\n",
      "\n",
      "Evaluated:\n",
      "metric = NDCG:top=10;type=Exp score = 0.409\n",
      "metric = DCG:top=10;type=Exp score = 8.253\n",
      "metric = MAP:top=10 score = 0.524\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = lgbm.LGBMRanker(\n",
    "    n_estimators=1000,         # maximum possible number of trees\n",
    "    objective='lambdarank',    # LambdaRank objective\n",
    "    metric='ndcg',             # metric used for early stopping\n",
    "    random_state=22\n",
    ")\n",
    "\n",
    "# Fit\n",
    "start = timer()\n",
    "model.fit(X_train, y_train, group=g_train,\n",
    "    eval_set=[(X_test, y_test)], eval_group=[g_test], eval_at=[10],\n",
    "    callbacks=[lgbm.early_stopping(stopping_rounds=10, first_metric_only=True)],\n",
    ")\n",
    "elapsed = timer() - start\n",
    "print(f\"Model fit: num_trees = {model.best_iteration_} elapsed = {elapsed:.3f}\")\n",
    "\n",
    "# Predict\n",
    "y_hat_test = model.predict(X_test)\n",
    "print(f\"Predicted: y_hat_test.shape = {y_hat_test.shape}\")\n",
    "\n",
    "# Compute metrics on test\n",
    "print(\"\\nEvaluated:\")\n",
    "compute_metrics(y_test, y_hat_test, q_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f9d6c3-80d6-4a6a-ab23-f67b70fc883f",
   "metadata": {},
   "source": [
    "Сравним результаты:\n",
    "\n",
    "- RMSE модель выбила NDCG@10 = **0.408**\n",
    "- а LambdaRank выбивает NDCG@10 = **0.409**\n",
    "\n",
    "Т.е. в данном случае мы не видим \"из коробки\" преимущества pairwise-подхода над poinwise, во всяком случае без дополнительного тюнинга гиперпараметров. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8806742a-7f08-4cec-a3a7-dd082097bef4",
   "metadata": {},
   "source": [
    "Также, обратим внимание, что абсолютные значения NDCG@10 даже у LambdaRank хуже того, что выбивала простая регрессия в catboost (**0.419**), не говоря уже о YetiRank!\n",
    "\n",
    "Видно, что \"из коробки\" catboost показывает значительно лучшие результаты, чем lightgbm.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
